import os
import shutil
from app_modules.overwrites import postprocess
from app_modules.presets import *
from clc.langchain_application import LangChainApplication
import streamlit as st
from streamlit_chat import message as message_chat

MAX_TURNS = 20
MAX_BOXES = MAX_TURNS * 2

st.set_page_config(page_title='Chinese-LangChain', layout='wide', initial_sidebar_state='auto')

# ä¿®æ”¹æˆè‡ªå·±çš„é…ç½®ï¼ï¼ï¼
class LangChainCFG:
    llm_model_name = 'THUDM/chatglm-6b-int4-qe'  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    # llm_model_name = 'THUDM/chatglm-6b'  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    embedding_model_name = 'GanymedeNil/text2vec-large-chinese'  # æ£€ç´¢æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    vector_store_path = './cache'
    docs_path = './docs'
    kg_vector_stores = {
        'é»˜è®¤çŸ¥è¯†åº“': './cache',
        'ä¸­æ–‡ç»´åŸºç™¾ç§‘': './cache/zh_wikipedia',
        'å¤§è§„æ¨¡é‡‘èç ”æŠ¥': './cache/financial_research_reports',        
    }  # å¯ä»¥æ›¿æ¢æˆè‡ªå·±çš„çŸ¥è¯†åº“ï¼Œå¦‚æœæ²¡æœ‰éœ€è¦è®¾ç½®ä¸ºNone
    # kg_vector_stores=None
    patterns = ['æ¨¡å‹é—®ç­”', 'çŸ¥è¯†åº“é—®ç­”']  #
    n_gpus=1
    # è¯·è¾“å…¥GOOLGE SERPER API KEYï¼Œå…è´¹è´¦å·ç”³è¯·åœ°å€ï¼šhttps://serper.dev/
    serper_api_key = "8102810aa7731f849ecb415acc225a51f906c66f"
    # å¯¹æœç´¢ç»“æœåˆ†æçš„è¯å‘é‡æ–‡ä»¶
    em_data_dir = "/root/emdata/"

def get_file_list():
    if not os.path.exists("docs"):
        return []
    return [f for f in os.listdir("docs")]

@st.cache_resource
def init_application():
    config = LangChainCFG()
    application = LangChainApplication(config)
    application.source_service.init_source_vector()
    file_list = get_file_list()
    print("=========================== ğŸ˜ƒ Enjoy your journey and be lucky! =========================================")
    print("\n")
    return config, application, file_list

if "application" not in st.session_state:
    st.session_state.config, st.session_state.application, st.session_state.file_list = init_application()

def upload_file(file):
    if not os.path.exists("docs"):
        os.mkdir("docs")
    file_list = st.session_state.file_list
    application = st.session_state.application
    filename = os.path.basename(file.name)
    shutil.move(file.name, "docs/" + filename)
    # file_listé¦–ä½æ’å…¥æ–°ä¸Šä¼ çš„æ–‡ä»¶
    file_list.insert(0, filename)
    application.source_service.add_document("docs/" + filename)
    return gr.Dropdown.update(choices=file_list, value=filename)

def set_knowledge(kg_name, history):
    config = st.session_state.config
    application = st.session_state.application
    try:
        application.source_service.load_vector_store(config.kg_vector_stores[kg_name])
        msg_status = f'{kg_name}çŸ¥è¯†åº“å·²æˆåŠŸåŠ è½½'
    except Exception as e:
        print(e)
        msg_status = f'{kg_name}çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½'
    return history + [[None, msg_status]]

def clear_session():
    st.session_state['gpt_history'] = []
    st.session_state['human_history'] = []  

def predict(input,
            large_language_model,
            embedding_model,
            top_k,
            use_web,
            use_pattern,
            history=None):
    # print(large_language_model, embedding_model)
    print('\n-------------------------------------------------')
    print(input)
    print('-------------------------------------------------')
    if history == None:
        history = []
    application = st.session_state.application

    if use_web == 'ä½¿ç”¨':
        web_content = application.source_service.search_web(query=input)
    else:
        web_content = ''
    search_text = ''
    result = ''

    # with container:
    #     message_chat(input, avatar_style="big-smile", key=str(len(history)) + "_user")                                

    if use_pattern == 'æ¨¡å‹é—®ç­”':        
        gen = application.get_llm_answer(query=input, web_content=web_content, history=history, use_stream=1)
        search_text = web_content   
        try:                 
            st.write("AIæ­£åœ¨å›å¤:")
            with st.empty():
                while True:
                    try:
                        result = next(gen)  # è·å–ä¸‹ä¸€ä¸ªæ•°æ®
                        history = application.llm_service.history
                        _, response = history[-1] 
                        st.write(response)
                    except StopIteration:  # å½“æ‰€æœ‰çš„æ•°æ®éƒ½è¢«éå†å®Œï¼Œnextå‡½æ•°ä¼šæŠ›å‡ºStopIterationçš„å¼‚å¸¸
                        st.session_state['human_history'].append((input, response))
                        # å®Œæˆåæ‰“å°æœ€ç»ˆçš„å›ç­”
                        history = application.llm_service.history                        
                        _, response = history[-1]
                        print('--------------- LLM Final Answer ----------------')
                        print(response)
                        break
        except KeyboardInterrupt:
            print("\nCtrl+C is pressed...")

        return '', history, history, search_text

    else:
        resp = application.get_knowledge_based_answer(
            query=input,
            history_len=1,
            temperature=0.1,
            top_p=0.9,
            top_k=top_k,
            web_content=web_content,
            chat_history=history
        )

        st.write("AIæ­£åœ¨å›å¤:")
        st.write(resp['result'])
        print('--------------- LLM Final Answer ----------------')
        print(resp['result'])
        st.session_state['human_history'].append((input, resp['result']))

        # history.append((input, resp['result']))
        history = application.llm_service.history

        for idx, source in enumerate(resp['source_documents'][:4]):
            sep = f'----------ã€çŸ¥è¯†åº“æœç´¢ç»“æœ{idx + 1}ï¼šã€‘---------------\n'
            search_text += f'{sep}\n{source.page_content}\n\n'
        print(search_text)
        search_text += "----------ã€ç½‘ç»œæ£€ç´¢å†…å®¹ã€‘-----------\n"
        search_text += web_content
        return '', history, history, search_text


# åœ¨è¿™é‡Œè¯»å–CSSæ–‡ä»¶å¹¶ä½¿ç”¨st.markdownæ¥è®¾ç½®æ ·å¼å¯èƒ½ä¸ä¼šæˆåŠŸï¼Œå› ä¸ºStreamlitçš„å®‰å…¨ç­–ç•¥å¯èƒ½ä¼šé˜»æ­¢å†…è”CSS

st.title("Chinese-LangChain")

embedding_model = st.sidebar.selectbox("Embedding model", ["text2vec-base"], 0)

large_language_model = st.sidebar.selectbox("large language model", ["ChatGLM-6B"], 0)

top_k = st.sidebar.slider("æ£€ç´¢top-kæ–‡æ¡£", 1, 20, 4, 1)

use_web = st.sidebar.radio("web search", ["ä½¿ç”¨", "ä¸ä½¿ç”¨"], 1)

use_pattern = st.sidebar.radio("æ¨¡å¼", ['æ¨¡å‹é—®ç­”', 'çŸ¥è¯†åº“é—®ç­”'], 0)

kg_name = st.sidebar.radio("çŸ¥è¯†åº“", list(st.session_state.config.kg_vector_stores.keys()), 0)

if st.sidebar.button("åŠ è½½çŸ¥è¯†åº“"):
    set_knowledge(kg_name)
    st.success("çŸ¥è¯†åº“å·²åŠ è½½")

uploaded_file = st.sidebar.file_uploader("å°†æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“ï¼Œå†…å®¹è¦å°½é‡åŒ¹é…", type=['.txt', '.md', '.docx', '.pdf'])

if uploaded_file is not None:
    upload_file(uploaded_file)
    st.success("æ–‡ä»¶å·²ä¸Šä¼ ")

# æ¨¡å‹çœ‹åˆ°çš„å¯¹è¯å†å²{prompt, response}
if 'gpt_history' not in st.session_state:
    st.session_state['gpt_history'] = []

# äººçœ‹åˆ°çš„å¯¹è¯å†å²{query, response}
if 'human_history' not in st.session_state:
    st.session_state['human_history'] = []    

container = st.container()
message = st.text_input('è¯·è¾“å…¥é—®é¢˜')

# åˆ›å»ºä¸¤åˆ—
col1, col2 = st.columns(2)

# åœ¨æ¯åˆ—ä¸­æ·»åŠ ä¸€ä¸ªæŒ‰é’®
send_button = col1.button('ğŸš€   å‘ é€    ')
clear_button = col2.button('ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯')

if "clear_clicked" not in st.session_state:
    st.session_state["clear_clicked"] = False

if clear_button or st.session_state["clear_clicked"]:
    st.success("å†å²å¯¹è¯å·²æ¸…é™¤")
    clear_session()    
    st.session_state["clear_clicked"] = False

# æ˜¾ç¤ºå†å²å¯¹è¯å†…å®¹
with container:
    human_history = st.session_state['human_history']
    if len(human_history) > 0:
        if len(human_history) > MAX_BOXES:
            human_history = human_history[-MAX_TURNS:]
        for i, (query, response) in enumerate(human_history):
            message_chat(query, avatar_style="big-smile", key=str(i) + "_user")
            message_chat(response, avatar_style="bottts", key=str(i))

# ç‚¹å‡»å‘é€æŒ‰é’®ï¼Œæš‚ä¸å¤„ç†è¾“å…¥å›è½¦
if send_button:
    _, _, history, search_result = predict(message, large_language_model, embedding_model, top_k, use_web, use_pattern, st.session_state["gpt_history"])                
    st.session_state["gpt_history"] = history
    st.text_area('æœç´¢ç»“æœ', search_result, height=200)      

st.markdown("""æé†’ï¼š<br>
            [Chinese-LangChain](https://github.com/yanqiangmiffy/Chinese-LangChain) <br>
            æœ‰ä»»ä½•ä½¿ç”¨é—®é¢˜[Github IssueåŒº](https://github.com/yanqiangmiffy/Chinese-LangChain)è¿›è¡Œåé¦ˆ. <br>
            """, unsafe_allow_html=True)


