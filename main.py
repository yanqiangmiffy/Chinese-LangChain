import os
import shutil

import gradio as gr

from clc.langchain_application import LangChainApplication

os.environ["CUDA_VISIBLE_DEVICES"] = '1'


# ä¿®æ”¹æˆè‡ªå·±çš„é…ç½®ï¼ï¼ï¼
class LangChainCFG:
    llm_model_name = '../../pretrained_models/chatglm-6b-int4-qe'  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    embedding_model_name = '../../pretrained_models/text2vec-large-chinese'  # æ£€ç´¢æ¨¡å‹æ–‡ä»¶ or huggingfaceè¿œç¨‹ä»“åº“
    vector_store_path = './cache'
    docs_path = './docs'
    kg_vector_stores = {
        'ä¸­æ–‡ç»´åŸºç™¾ç§‘': '/root/GoMall/Knowledge-ChatGLM/cache/zh_wikipedia',
        'å¤§è§„æ¨¡é‡‘èç ”æŠ¥çŸ¥è¯†å›¾è°±': '/root/GoMall/Knowledge-ChatGLM/cache/financial_research_reports',
        'åˆå§‹åŒ–çŸ¥è¯†åº“': '/root/GoMall/Knowledge-ChatGLM/cache',
    }  # å¯ä»¥æ›¿æ¢æˆè‡ªå·±çš„çŸ¥è¯†åº“ï¼Œå¦‚æœæ²¡æœ‰éœ€è¦è®¾ç½®ä¸ºNone
    # kg_vector_stores=None


config = LangChainCFG()
application = LangChainApplication(config)


def get_file_list():
    if not os.path.exists("docs"):
        return []
    return [f for f in os.listdir("docs")]


file_list = get_file_list()


def upload_file(file):
    if not os.path.exists("docs"):
        os.mkdir("docs")
    filename = os.path.basename(file.name)
    shutil.move(file.name, "docs/" + filename)
    # file_listé¦–ä½æ’å…¥æ–°ä¸Šä¼ çš„æ–‡ä»¶
    file_list.insert(0, filename)
    application.source_service.add_document("docs/" + filename)
    return gr.Dropdown.update(choices=file_list, value=filename)


def set_knowledge(kg_name, history):
    try:
        application.source_service.load_vector_store(config.kg_vector_stores[kg_name])
        msg_status = f'{kg_name}çŸ¥è¯†åº“å·²æˆåŠŸåŠ è½½'
    except Exception as e:
        msg_status = f'{kg_name}çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½'
    return history + [[None, msg_status]]


def clear_session():
    return '', None


def predict(input,
            large_language_model,
            embedding_model,
            history=None):
    # print(large_language_model, embedding_model)
    print(input)
    if history == None:
        history = []
    resp = application.get_knowledge_based_answer(
        query=input,
        history_len=1,
        temperature=0.1,
        top_p=0.9,
        chat_history=history
    )
    history.append((input, resp['result']))
    search_text = ''
    for idx, source in enumerate(resp['source_documents'][:4]):
        sep = f'----------ã€æœç´¢ç»“æœ{idx+1}ï¼šã€‘---------------\n'
        search_text += f'{sep}\n{source.page_content}\n\n'
    print(search_text)
    return '', history, history, search_text


block = gr.Blocks()
with block as demo:
    gr.Markdown("""<h1><center>Chinese-LangChain</center></h1>
        <center><font size=3>
        </center></font>
        """)
    state = gr.State()

    with gr.Row():
        with gr.Column(scale=1):
            embedding_model = gr.Dropdown([
                "text2vec-base"
            ],
                label="Embedding model",
                value="text2vec-base")

            large_language_model = gr.Dropdown(
                [
                    "ChatGLM-6B-int4",
                ],
                label="large language model",
                value="ChatGLM-6B-int4")

            top_k = gr.Slider(1,
                              20,
                              value=2,
                              step=1,
                              label="å‘é‡åŒ¹é… top k",
                              interactive=True)
            kg_name = gr.Radio(['ä¸­æ–‡ç»´åŸºç™¾ç§‘',
                                'å¤§è§„æ¨¡é‡‘èç ”æŠ¥çŸ¥è¯†å›¾è°±',
                                'åˆå§‹åŒ–çŸ¥è¯†åº“'
                                ],
                               label="çŸ¥è¯†åº“",
                               value='ä¸­æ–‡ç»´åŸºç™¾ç§‘',
                               interactive=True)
            set_kg_btn = gr.Button("é‡æ–°åŠ è½½çŸ¥è¯†åº“")

            file = gr.File(label="å°†æ–‡ä»¶ä¸Šä¼ åˆ°æ•°æ®åº“",
                           visible=True,
                           file_types=['.txt', '.md', '.docx', '.pdf']
                           )

            file.upload(upload_file,
                        inputs=file,
                        outputs=None)
        with gr.Column(scale=4):
            with gr.Row():
                with gr.Column(scale=4):
                    chatbot = gr.Chatbot(label='Chinese-LangChain').style(height=400)
                    message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜')
                    with gr.Row():
                        clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                        send = gr.Button("ğŸš€ å‘é€")
                with gr.Column(scale=2):
                    search = gr.Textbox(label='æœç´¢ç»“æœ')
        set_kg_btn.click(
            set_knowledge,
            show_progress=True,
            inputs=[kg_name, chatbot],
            outputs=chatbot
        )
        # å‘é€æŒ‰é’® æäº¤
        send.click(predict,
                   inputs=[
                       message, large_language_model,
                       embedding_model, state
                   ],
                   outputs=[message, chatbot, state, search])

        # æ¸…ç©ºå†å²å¯¹è¯æŒ‰é’® æäº¤
        clear_history.click(fn=clear_session,
                            inputs=[],
                            outputs=[chatbot, state],
                            queue=False)

        # è¾“å…¥æ¡† å›è½¦
        message.submit(predict,
                       inputs=[
                           message, large_language_model,
                           embedding_model, state
                       ],
                       outputs=[message, chatbot, state, search])
    gr.Markdown("""æé†’ï¼š<br>
            [Chinese-LangChain](https://github.com/yanqiangmiffy/Chinese-LangChain) <br>
            æœ‰ä»»ä½•ä½¿ç”¨é—®é¢˜[Github IssueåŒº](https://github.com/yanqiangmiffy/Chinese-LangChain)è¿›è¡Œåé¦ˆ. <br>
            """)
demo.queue(concurrency_count=2).launch(
    server_name='0.0.0.0',
    server_port=8888,
    share=False,
    show_error=True,
    debug=True,
    enable_queue=True
)
